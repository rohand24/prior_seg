{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import pathlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "from skimage.util import random_noise\n",
    "import time\n",
    "import os\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from prior_dataloader import RetraceDataLoader, retrace_parser, retrace_parser_synth\n",
    "from calculus_dataloader import RetraceDataLoader, CalculusBase\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler\n",
    "from custom_unets import NestedUNet, U_Net, DeepNestedUNet\n",
    "from sync_batchnorm import SynchronizedBatchNorm2d, DataParallelWithCallback, convert_model\n",
    "# from kornia.losses import FocalLoss\n",
    "from pywick.losses import BCEDiceFocalLoss\n",
    "\n",
    "import glob2\n",
    "import pdb\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "rohan_unet = DeepNestedUNet(1,1)\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    rohan_unet = nn.DataParallel(rohan_unet)\n",
    "rohan_unet = rohan_unet.to(device)\n",
    "rohan_unet = convert_model(rohan_unet)\n",
    "rohan_unet = rohan_unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dict = torch.load('/home/rohan/prior_seg/models/prior_256/256_model_epoch_65.0_f1_0.8814.pth')\n",
    "print('Weight dict before = {} weights'.format(len(pretrained_dict)))\n",
    "model_dict = rohan_unet.state_dict()\n",
    "\n",
    "# filter unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if\n",
    "                       (k in model_dict) and (model_dict[k].shape == pretrained_dict[k].shape)}\n",
    "print('Weight dict after = {} weights'.format(len(pretrained_dict)))\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict) \n",
    "# 3. load the new state dict\n",
    "rohan_unet.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in list(rohan_unet.children()):\n",
    "    for layer in list(module.children())[:-1]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# for module in list(rohan_unet.children()):\n",
    "#     for layer in list(module.children()):\n",
    "#         for param in layer.parameters():\n",
    "#             print(layer, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "# summary(rohan_unet, input_size=(1,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed= 42\n",
    "mark4 = time.time()\n",
    "root_dir='/home/rohan/Datasets/prior_clean/train'\n",
    "calculus_clean_train = CalculusBase(root_dir, oversample=2, issynthetic=False, test_size=0.1,im_range='all', isTrain=True)\n",
    "calculus_clean_val = CalculusBase(root_dir, oversample=2, issynthetic=False, test_size=0.1,im_range='all', isTrain=False)\n",
    "print('Real dataload time {:.6f}'.format(time.time() - mark4))\n",
    "\n",
    "mark3 = time.time()\n",
    "synth_root_dir='/home/rohan/Datasets/synthetic_prior_clean/train'\n",
    "calculus_synthetic_train = CalculusBase(synth_root_dir, oversample=1, issynthetic=True, test_size=0.05,im_range=[0,88000], isTrain=True)\n",
    "calculus_synthetic_val = CalculusBase(synth_root_dir, oversample=1, issynthetic=True, test_size=0.05,im_range=[0,88000], isTrain=False)\n",
    "print('Synthetic dataload time {:.6f}'.format(time.time() - mark3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RetraceDataLoader(calculus_clean_train,\n",
    "                                           calculus_synthetic_train,\n",
    "                                  image_size=(384,384),\n",
    "                                  length = 'all',# pass 'all' for all\n",
    "                                  crop = True,\n",
    "                                  transform=None,\n",
    "                                 mode = 'train')\n",
    "\n",
    "\n",
    "val_dataset = RetraceDataLoader(calculus_clean_val,\n",
    "                                           None,\n",
    "                                  image_size=(384,384),\n",
    "                                  length = 'all',# pass 'all' for all\n",
    "                                  crop = True,\n",
    "                                  transform=None,\n",
    "                               mode = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler\n",
    "\n",
    "random_seed= 42\n",
    "batch_size = 256\n",
    "# def get_sampler(dataset, shuffle=False):\n",
    "    \n",
    "#     # Creating data indices for training and validation splits:\n",
    "#     dataset_size = len(dataset)\n",
    "#     indices = list(range(dataset_size))\n",
    "#     if shuffle :\n",
    "#         np.random.seed(random_seed)\n",
    "#         np.random.shuffle(indices)\n",
    "#     sampler = SequentialSampler(indices)\n",
    "    \n",
    "#     return sampler\n",
    "\n",
    "# # Creating PT data samplers and loaders:\n",
    "# train_sampler = get_sampler(train_dataset, shuffle=True)\n",
    "# valid_sampler = get_sampler(val_dataset, shuffle=False)\n",
    "\n",
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    worker_init_fn=worker_init_fn,\n",
    "    pin_memory = True,\n",
    "    drop_last =True\n",
    ")\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    worker_init_fn=worker_init_fn,\n",
    "    pin_memory = True,\n",
    "    drop_last =True\n",
    ")\n",
    "print ('Train size: ', len(trainloader))\n",
    "print ('Validation size: ', len(valloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import pdb\n",
    "import pandas as pd\n",
    "\n",
    "dataloaders = {'train': trainloader,'val':valloader}\n",
    "dataset_sizes = {'train':len(trainloader), 'val':len(valloader)}\n",
    "\n",
    "\n",
    "SMOOTH = 1e-6\n",
    "\n",
    "\n",
    "def dice_loss(input, target):\n",
    "    smooth = SMOOTH\n",
    "    \n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "def dice_score(input, target):\n",
    "    smooth = SMOOTH\n",
    "#     print(input.shape)\n",
    "#     ipdb.set_trace()\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return ((2. * intersection + smooth) /\n",
    "              (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "def dice_per_channel(inputs, target):\n",
    "    \n",
    "    dice_ch = 0.0\n",
    "    for i in range(0, inputs.shape[1]):\n",
    "        inp = inputs[:,i,:,:]\n",
    "        inp = inp.contiguous()\n",
    "        targs = target[:,i,:,:]\n",
    "        targs = targs.contiguous()\n",
    "        dice_chl = dice_score(inp,targs)\n",
    "        dice_ch +=dice_chl\n",
    "    \n",
    "    return dice_ch / (inputs.shape[1]-1)\n",
    "\n",
    "def dice_per_image(inputs, target):\n",
    "    \n",
    "    dice_img = 0.0\n",
    "    for i in range(0, inputs.shape[0]):\n",
    "        inp = inputs[i,:,:,:]\n",
    "        inp = inp.contiguous()\n",
    "        targs = target[i,:,:,:]\n",
    "        targs = targs.contiguous()\n",
    "        dice_im = dice_score(inp,targs)\n",
    "        dice_img +=dice_im\n",
    "    \n",
    "    return dice_img / (inputs.shape[0]-1)\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, writer, num_epochs=15):\n",
    "    start = time.time()\n",
    "    save_dict={}\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 10.0\n",
    "    best_iou = 0.0\n",
    "    best_f1 = 0.0\n",
    "    best_f1_ch = 0.0\n",
    "    best_f1_img = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        ep_start = time.time()\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        lrate = scheduler.get_lr()[0]\n",
    "        writer.add_scalar('Learning Rate', lrate, epoch)\n",
    "        print('LR {:.5f}'.format(lrate))\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            running_loss = 0.0\n",
    "            running_ious = 0.0\n",
    "            running_f1 = 0.0\n",
    "            running_f1_ch = 0.0\n",
    "            running_f1_img = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "#                 ipdb.set_trace()\n",
    "                inputs = data['image'][:,:,:,:]\n",
    "                labels = data['masks'][:,:,:,:]\n",
    "#               labels = labels.unsqueeze(0)\n",
    "#                 labels = labels.float()\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                labels = labels.type(torch.cuda.FloatTensor)\n",
    "                 # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "#               torch.autograd.set_detect_anomaly(True)\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    preds = torch.sigmoid(outputs)\n",
    "                    \n",
    "                    bce = criterion(preds, labels)\n",
    "                    diceloss = dice_loss(preds,labels)\n",
    "                    loss = bce * 0.5 + diceloss * (1 - 0.5)\n",
    "                    \n",
    "                    bin_preds = preds.clone().detach()\n",
    "                    bin_preds[bin_preds<=0.5]= 0.0\n",
    "                    bin_preds[bin_preds>0.5]= 1.0\n",
    "                    \n",
    "#                     ipdb.set_trace()\n",
    "                    f1 = dice_score(bin_preds, labels)\n",
    "                    f1_img = dice_per_image(bin_preds,labels)\n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    # statistics\n",
    "                    running_loss += loss.data.cpu().numpy() # * inputs.size(0)\n",
    "                    running_f1 += f1\n",
    "                    running_f1_img += f1_img\n",
    "                    \n",
    "            torch.cuda.empty_cache()\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_f1 = running_f1 / dataset_sizes[phase]\n",
    "            epoch_f1_img = running_f1_img / dataset_sizes[phase]\n",
    "\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "                writer.add_scalar('Hard_Dice/train', epoch_f1, epoch)\n",
    "                writer.add_scalar('Hard_Dice_per_image/train', epoch_f1_img, epoch)\n",
    "            else:\n",
    "                writer.add_scalar('Loss/val', epoch_loss, epoch)\n",
    "                writer.add_scalar('Hard_Dice/val', epoch_f1, epoch)\n",
    "                writer.add_scalar('Hard_Dice_per_image/val', epoch_f1_img, epoch)\n",
    "\n",
    "            print('{} Loss: {:.4f} F1: {:.4f} F1/img: {:.4f}'.format(phase, epoch_loss, epoch_f1, epoch_f1_img))#F1/ch: {:.4f} epoch_f1_ch,\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_f1 > best_f1:\n",
    "                best_loss = epoch_loss\n",
    "                best_f1 = epoch_f1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, '/home/rohan/prior_seg/models/calculus_frozen/calculus_256_model_epoch_{:.1f}_f1_{:.4f}.pth'.format(epoch, best_f1))\n",
    "            writer.add_scalar('Hard_Dice/best_val', best_f1, epoch)\n",
    "            \n",
    "\n",
    "        print('Epoch completed in {:.4f} seconds'.format(time.time()-ep_start))\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Val F1: {:4f}'.format(best_f1))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = optim.Adam(rohan_unet.parameters(), lr=0.0001)\n",
    "writer = SummaryWriter(log_dir='/home/rohan/prior_seg/logs/calculus_frozen', filename_suffix = '_calculus_256')\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.85)\n",
    "# rohan_unet.load_state_dict(torch.load('/home/rohan/caries_seg/models/test_model/128_nestedunet_model_epoch_0.0000_f1_0.1858.pth'))\n",
    "model_trained = train_model(rohan_unet, criterion, optimizer, exp_lr_scheduler, writer = writer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
