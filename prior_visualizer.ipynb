{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import pathlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "from skimage.util import random_noise\n",
    "import time\n",
    "import os\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from prior_dataloader import RetraceDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler\n",
    "from custom_unets import NestedUNet, U_Net\n",
    "from sync_batchnorm import SynchronizedBatchNorm2d, DataParallelWithCallback, convert_model\n",
    "\n",
    "import glob2\n",
    "import pdb\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1 GPUs!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "rohan_unet = NestedUNet(1,33)\n",
    "if torch.cuda.device_count() > 0:\n",
    "      print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "      # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "      rohan_unet = nn.DataParallel(rohan_unet)\n",
    "rohan_unet = rohan_unet.to(device)\n",
    "rohan_unet = convert_model(rohan_unet)\n",
    "rohan_unet.load_state_dict(torch.load('/home/rohan/prior_seg/models/prior_model1/1st_epoch_42.0000_f1_0.8835.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "# new_state_dict = OrderedDict()\n",
    "# state_dict = torch.load('/home/rohan/prior_seg/models/prior_model1/1st_epoch_42.0000_f1_0.8835.pth',  map_location=device)\n",
    "# for k, v in state_dict.items():\n",
    "#     name = k[7:] # remove `module.`\n",
    "#     new_state_dict[name] = v\n",
    "# # load params\n",
    "# rohan_unet.load_state_dict(new_state_dict)\n",
    "# unet_model = convert_model(unet_model)\n",
    "# unet_model = unet_model.to(device)\n",
    "rohan_unet = rohan_unet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "# summary(rohan_unet, input_size=(1,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/rohan/Datasets/prior_clean/train/'\n",
    "syn_root_dir = '/home/rohan/Datasets/synthetic_prior_clean/train/'\n",
    "\n",
    "# prior_data = RetraceDataLoader(root_dir, syn_root_dir, length = 100)\n",
    "teeth_dataset = RetraceDataLoader(root_dir=root_dir,\n",
    "                                  root_dir_synth=None, #syn_root_dir,\n",
    "                                  image_size=(128,128),\n",
    "                                  length = 'all',# pass 'all' for all\n",
    "                                  transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "validation_split = .1\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "batch_size = 48\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(teeth_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    teeth_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    shuffle=False,\n",
    "    sampler=train_sampler,\n",
    "    worker_init_fn=worker_init_fn,\n",
    "    pin_memory = True,\n",
    "    drop_last =True\n",
    ")\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    teeth_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    shuffle=False,\n",
    "    sampler=valid_sampler,\n",
    "    worker_init_fn=worker_init_fn,\n",
    "    pin_memory = True,\n",
    "    drop_last =True\n",
    ")\n",
    "print ('Train size: ', len(trainloader))\n",
    "print ('Validation size: ', len(valloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import pdb\n",
    "import pandas as pd\n",
    "\n",
    "dataloaders = {'train': trainloader,'val':valloader}\n",
    "dataset_sizes = {'train':len(trainloader), 'val':len(valloader)}\n",
    "\n",
    "\n",
    "SMOOTH = 1e-6\n",
    "\n",
    "\n",
    "def dice_loss(input, target):\n",
    "    smooth = SMOOTH\n",
    "    \n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "def dice_score(input, target):\n",
    "    smooth = SMOOTH\n",
    "#     print(input.shape)\n",
    "#     ipdb.set_trace()\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return ((2. * intersection + smooth) /\n",
    "              (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "def dice_per_channel(inputs, target):\n",
    "    \n",
    "    dice_ch = 0.0\n",
    "    for i in range(1, inputs.shape[1]):\n",
    "        inp = inputs[:,i,:,:]\n",
    "        inp = inp.contiguous()\n",
    "        targs = target[:,i,:,:]\n",
    "        targs = targs.contiguous()\n",
    "        dice_chl = dice_score(inp,targs)\n",
    "        dice_ch +=dice_chl\n",
    "    \n",
    "    return dice_ch / (inputs.shape[1]-1)\n",
    "\n",
    "\n",
    "def infer_model(model, output_df, num_epochs=15):\n",
    "    start = time.time()\n",
    "    out_dict={}\n",
    "    running_f1=0.0\n",
    "    running_f1_ch=0.0\n",
    "    running_img_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "#                 ipdb.set_trace()\n",
    "                inputs = data['image'][:,:,:,:]\n",
    "                labels = data['masks'][:,:,:,:]\n",
    "#               labels = labels.unsqueeze(0)\n",
    "#                 labels = labels.float()\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                labels = labels.type(torch.cuda.FloatTensor)\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    preds = torch.sigmoid(outputs)\n",
    "                    \n",
    "                    bin_preds = preds.clone().detach()\n",
    "                    bin_preds[bin_preds<=0.5]= 0.0\n",
    "                    bin_preds[bin_preds>0.5]= 1.0\n",
    "                    f1 = dice_score(bin_preds, labels)\n",
    "                    f1_ch = dice_per_channel(bin_preds, labels)\n",
    "                    running_f1 += f1\n",
    "                    running_f1_ch += f1_ch\n",
    "                    \n",
    "                    img_f1 = 0.0\n",
    "                    for img,lbl,pred,bin_pred in zip(inputs, labels, preds, bin_preds):\n",
    "                        out_dict = {}\n",
    "                        hard_dice = dice_score(bin_pred, lbl)\n",
    "                        img_f1 += hard_dice\n",
    "#                         ipdb.set_trace()\n",
    "                        out_dict['hard_dice'] = hard_dice.detach().cpu().numpy()\n",
    "                        out_dict['input_image'] = img[0,:,:].detach().cpu().numpy()\n",
    "                        out_dict['mask'] = lbl.detach().cpu().numpy()\n",
    "                        out_dict['pred'] = bin_pred.detach().cpu().numpy()\n",
    "                        \n",
    "#                         out_dict['act_image'] = act_img.numpy()\n",
    "#                         out_dict['act_mask'] = act_mask.numpy()\n",
    "#                         out_dict['act_tmask'] = act_tmask.numpy()\n",
    "                        \n",
    "                        output_df = output_df.append(out_dict, ignore_index=True)\n",
    "                    batch_img_f1 = img_f1 / labels.shape[0]\n",
    "                    running_img_f1 +=batch_img_f1\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "#         ipdb.set_trace()\n",
    "        epoch_f1 = running_f1 / dataset_sizes[phase]\n",
    "        epoch_f1_ch = running_f1_ch / dataset_sizes[phase]\n",
    "        img_f1 = running_img_f1 / dataset_sizes[phase]\n",
    "        print('F1: {:.4f} '.format(epoch_f1))\n",
    "        print('F1 per channel: {:.4f} '.format(epoch_f1_ch))\n",
    "        print('Mean Image F1: {:.4f} '.format(img_f1))\n",
    "        print('Epoch completed in {:.4f} seconds'.format(time.time()-start))\n",
    "        torch.cuda.empty_cache()\n",
    "    return model, output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_df = pd.DataFrame(columns=['hard_dice','input_image', 'mask', 'pred'])\n",
    "model_out, output_df = infer_model(rohan_unet, output_df, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = output_df.sort_values('hard_dice', ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(columns = sorted_df.columns)\n",
    "\n",
    "for i in range(len(sorted_df)):\n",
    "    if len(np.unique(sorted_df['mask'][i]))>1:\n",
    "        new_df = new_df.append({'hard_dice':sorted_df.hard_dice[i], 'input_image':sorted_df.input_image[i], 'mask':sorted_df['mask'][i], 'pred':sorted_df.pred[i]}, ignore_index=True)\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_100 = sorted_df[:50]\n",
    "top_100 = sorted_df[-50:]\n",
    "conditional_100 = sorted_df[sorted_df.hard_dice>0.6][:50]\n",
    "has_caries_top100 = new_df[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_df.pred[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_numpy = preds.detach().numpy()\n",
    "def plotter(ax, input_img, gt_mask, pred, f1, idx):\n",
    "    \n",
    "    ax[idx,0].imshow(input_img, cmap='gray')\n",
    "    \n",
    "    ax[idx,1].imshow(np.squeeze(np.argmax(np.flip(gt_mask,axis=0),axis=0)) ,vmin=0,vmax=32 , cmap='flag')\n",
    "    \n",
    "    ax[idx,2].imshow(np.squeeze(np.argmax(np.flip(pred,axis=0),axis=0)) ,vmin=0,vmax=32 , cmap='flag')\n",
    "    \n",
    "    if idx==0:\n",
    "        ax[idx,0].set_title('Input Image')\n",
    "        ax[idx,1].set_title('Ground Truth Mask')\n",
    "        ax[idx,2].set_title('Prediction Image \\n Dice = {:.4f}'.format(f1))\n",
    "    else:\n",
    "        ax[idx,2].set_title('Dice = {:.4f}'.format(f1))\n",
    "    \n",
    "\n",
    "def plot_df(df, title):\n",
    "    f, ax = plt.subplots(len(df) , 3, figsize=(24,250))\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        input_img = df.input_image[df.index[i]]\n",
    "        gt_mask = df['mask'][df.index[i]]\n",
    "        pred = df.pred[df.index[i]]\n",
    "        f1 = df.hard_dice[df.index[i]]\n",
    "        \n",
    "        \n",
    "        plotter(ax, input_img, gt_mask, pred, f1, i)\n",
    "    f.tight_layout()\n",
    "    f.suptitle(title,y=1.001, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_df(low_100, 'Images with low hard dice value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df(top_100, 'Images with high hard dice value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
